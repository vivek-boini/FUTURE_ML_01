# -*- coding: utf-8 -*-
"""ML_task_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yJr0z0XoNETChb4C5GRnP2IUq8OP8dxz

# ðŸ›’ **Sales Forecasting Dashboard using Superstore Dataset**

# **ðŸ“Œ Problem Statement**

Build a forecasting model and an interactive dashboard to predict future sales trends for a retail business, using historical data from the Superstore dataset.

The project aims to help the business:

Plan inventory

Optimize marketing strategies

Understand seasonality patterns

Forecast expected sales over the next several months

# **ðŸŽ¯ Objectives**

âœ… Predict future total sales using historical daily data

âœ… Visualize:

Forecast vs. Actual Sales

Trend & Seasonality Components

Breakdown by Product Category / Region

âœ… Build an interactive Power BI dashboard to present insights

# **ðŸ§° Tools & Technologies**

Python

pandas â†’ Data cleaning & preprocessing

prophet â†’ Time-series forecasting

matplotlib, seaborn â†’ Data visualization

Power BI â†’ Dashboard creation & business insights

# ðŸ“¦ **Dataset**

Superstore Sales Dataset
Contains order-level data: Order Date, Sales, Profit, Category, Sub-Category, Region, etc.

# Import the necessary libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Knowing the Dataset**"""

# Load dataset with correct encoding
df = pd.read_csv("/content/Superstore.csv", encoding="ISO-8859-1")
df.head()

"""**ðŸ§¹ Data overview & quality check**"""

df.shape

df.describe()

df.duplicated()

"""# **Data cleaning**

**Before exploring and modeling the data, we do some basic data wrangling to clean and simplify the dataset:**

Remove columns that are irrelevant, have only one unique value, or don't help in forecasting:

Row ID (internal index)

Order ID

Customer ID

Customer Name

Postal Code

Product ID

(high cardinality, not useful for aggregate sales forecasting)
"""

df.drop(columns=[
    'Row ID',
    'Order ID',
    'Customer ID',
    'Customer Name',
    'Postal Code',
    'Product ID'
], inplace=True)

"""# Convert date columns & create time-based features
To prepare the data for time series forecasting and deeper analysis, we:

Convert Order Date and Ship Date columns to datetime format
Create new time-based columns from Order Date:
year â€” year of the order
month â€” month of the order
week â€” week number in the year
day â€” day of the month
day of week â€” day of the week (0=Monday, 6=Sunday)
These features help us explore trends, seasonality, and patterns over time.
"""

# Convert Order Date & Ship Date to datetime
df['Order Date'] = pd.to_datetime(df['Order Date'])
df['Ship Date'] = pd.to_datetime(df['Ship Date'])

df['year'] = df['Order Date'].dt.year
df['month'] = df['Order Date'].dt.month
df['week'] = df['Order Date'].dt.isocalendar().week
df['day'] = df['Order Date'].dt.day
df['day of week'] = df['Order Date'].dt.day_of_week

df.drop_duplicates(inplace=True)
df.duplicated().sum()

"""# Exploratory Data Analysis (EDA)
**To better understand the data and find useful patterns, we explore:**

Correlations between numerical features

Distribution of sales and profit

Total sales by category, sub-category, region, and segment

Sales trend over time

These insights guide our feature selection and forecasting.
"""

# Correlation heatmap for numerical features
plt.figure(figsize=(8,5))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# Distribution of Sales
plt.figure(figsize=(8,5))
sns.histplot(df['Sales'], bins=50, kde=True)
plt.title("Sales Distribution")
plt.show()

# Distribution of Profit
plt.figure(figsize=(8,5))
sns.histplot(df['Profit'], bins=50, kde=True)
plt.title("Profit Distribution")
plt.show()

# Total Sales by Category
plt.figure(figsize=(7,5))
sns.barplot(x='Category', y='Sales', data=df, estimator=sum)
plt.title("Total Sales by Category")
plt.show()

# Total Sales by Sub-Category
plt.figure(figsize=(12,5))
sns.barplot(x='Sub-Category', y='Sales', data=df, estimator=sum)
plt.xticks(rotation=45)
plt.title("Total Sales by Sub-Category")
plt.show()

# Total Sales by Region
plt.figure(figsize=(7,5))
sns.barplot(x='Region', y='Sales', data=df, estimator=sum)
plt.title("Total Sales by Region")
plt.show()

# Total Sales by Segment
plt.figure(figsize=(7,5))
sns.barplot(x='Segment', y='Sales', data=df, estimator=sum)
plt.title("Total Sales by Segment")
plt.show()

"""# Aggregating Data
We aggregate total sales by Order Date because time series models like Prophet require:

One row per time point (e.g., per day)

A single target value per date (total daily sales)

This transforms our raw transactional data into a clean daily time series for forecasting.

**Why do we rename columns to ds and y?**

Prophet requires the input DataFrame to have:

ds: the datetime column

y: the numeric target variable

By renaming Order Date â†’ ds and Sales â†’ y, we prepare the dataset for modeling.
"""

daily_sales = df.groupby('Order Date')['Sales'].sum().reset_index()
daily_sales.rename(columns={'Order Date': 'ds', 'Sales': 'y'}, inplace=True)

daily_sales = df.groupby('Order Date')['Sales'].sum().reset_index()

plt.figure(figsize=(14,6))
plt.plot(daily_sales['Order Date'], daily_sales['Sales'])
plt.title('Daily Sales Over Time')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.show()

"""# **Handling Outliers in Profit**

Negative profits are valid as they represent real losses,
but extremely high profits may distort our analysis.

We used the Interquartile Range (IQR) method to cap profits above the calculated upper bound, while keeping other values unchanged.
"""

Q1 = df['Profit'].quantile(0.25)
Q3 = df['Profit'].quantile(0.75)

IQR = Q3 - Q1

upper_bound = Q3 + 1.5 * IQR

df['Profit'] = np.where(df['Profit'] > upper_bound, upper_bound, df['Profit'])

"""# **Model Training**"""

from prophet import Prophet

daily_sales = daily_sales.rename(columns={'Order Date': 'ds', 'Sales': 'y'})
holidays = pd.DataFrame({
    'holiday': ['Black Friday']*4 + ['Christmas']*4 + ['New Year']*4,
    'ds': pd.to_datetime([
        '2014-11-28', '2015-11-27', '2016-11-25', '2017-11-24',
        '2014-12-25', '2015-12-25', '2016-12-25', '2017-12-25',
        '2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01'
    ]),
    'lower_window': 0,
    'upper_window': 1
})


model = Prophet(holidays=holidays,
                yearly_seasonality=True,
                weekly_seasonality=True,
                daily_seasonality=False)

model.add_seasonality(name='monthly', period=30.5, fourier_order=5)

daily_sales['y'] = np.log1p(daily_sales['y'])
model.fit(daily_sales)

future = model.make_future_dataframe(periods=180)
forecast = model.predict(future)

fig1 = model.plot(forecast)
plt.title('Forecast of Daily Sales')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.show()

fig2 = model.plot_components(forecast)
plt.show()

"""# **Model Evaluation**
To evaluate the forecasting performance, we used Prophet's built-in cross-validation method with a horizon of 9â€“13 days and computed standard metrics:

MAPE (Mean Absolute Percentage Error)

RMSE / MAE: show stable and reasonable absolute errors.

Coverage
These results suggest the model captures the main sales trends reasonably well, providing a reliable forecast for the business.
"""

from prophet.diagnostics import cross_validation, performance_metrics
# horizon: how far into the future to forecast each time (e.g., 90 days)
# initial: size of the initial training period (e.g., 730 days ~ 2 years)
# period: spacing between cutoff dates

df_cv = cross_validation(model,
                         initial='730 days',
                         period='180 days',
                         horizon = '90 days')
df_p = performance_metrics(df_cv)
print(df_p.head())

from prophet.plot import plot_cross_validation_metric

fig = plot_cross_validation_metric(df_cv, metric='mape')
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Merge forecast with actuals
df_merged = pd.merge(daily_sales, forecast[['ds', 'yhat']], left_on='ds', right_on='ds', how='inner')

# Calculate error metrics
mae = mean_absolute_error(df_merged['y'], df_merged['yhat'])
rmse = np.sqrt(mean_squared_error(df_merged['y'], df_merged['yhat']))
mape = np.mean(np.abs((df_merged['y'] - df_merged['yhat']) / df_merged['y'])) * 100

print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAPE: {mape:.2f}%")

# Save combined dataset for Power BI
df_merged.to_csv('historical_and_forecast_sales.csv', index=False)

"""# **Exporting Dataset**"""

# Get last available date in historical data
last_date = daily_sales['ds'].max()

# Filter only future forecasted dates
future_forecast = forecast[forecast['ds'] > last_date]

# Save future forecast
future_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].to_csv('future_forecasted_sales.csv', index=False)

# Save historical daily sales
daily_sales.to_csv('historical_daily_sales.csv', index=False)

# Save Error matrics
metrics = {
    "Metric": ["MAE", "RMSE", "MAPE"],
    "Value": [mae,rmse, mape]
}
metrics_df = pd.DataFrame(metrics)
metrics_df.to_csv("forecast_metrics.csv", index=False)

